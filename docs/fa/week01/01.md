---
lang-ref: ch.01
title: هفته‌ی ۱
lang: fa
translation-date: 12 July 2022
translator: Nima Afshar
---

<!-- Lecture part A -->

## بخش الف ارائه

<!--
We discuss the motivation behind deep learning. We begin with the history and inspiration of deep learning. Then we discuss the history of pattern recognition and introduce gradient descent and its computation by backpropagation. Finally, we discuss the hierarchical representation of the visual cortex.
-->

در مورد انگیزه پشت یادگیری عمیق بحث می‌کنیم. ابتدا با تاریخچه و مواردی که یادگیری عمیق از آنها الهام گرفته شروع می‌کنیم، 
سپس به تاریخچه‌ی تشخیص الگو
(*pattern recognition*)
می‌پردازیم و
گرادیان کاهشی
(*gradient descent*)
و روش محاسبه‌ی آن توسط
پس‌انتشار
(*backpropagation*)
را معرفی می‌کنیم.
و در نهایت در مورد اینکه چگونه قشر بینایی مغز تصاویر را به صورت سلسله مراتبی ارائه می‌کند بحث خواهیم کرد.


<!-- Lecture part B -->

## بخش ب ارائه

<!--
We first discuss the evolution of CNNs, from Fukushima to LeCun to AlexNet. We then discuss some applications of CNN's, such as image segmentation, autonomous vehicles, and medical image analysis. We discuss the hierarchical nature of deep networks and the attributes of deep networks that make them advantageous. We conclude with a discussion of generating and learning features/representations.
-->
ابتدا سیر تکامل شبکه‌های کانولوشنی، از مدل 
*Fukushima*
تا مدل
*LeCun*
و در نهایت
*AlexNet*
را توضیح خواهیم داد.
پس از آن در مورد چند کاربرد این نوع از شبکه‌ها مثل
بخش‌بندی عکس
(*image segmentation*)،
اتوموبیل‌های خودران
و تحلیل عکس‌های پزشکی بحث خواهیم کرد.
در مورد طبیعت سلسله‌مراتبی شبکه‌های عصبی و ویژگی‌هایی که آنها را نسبت به بقیه مدل‌ها بهتر می‌کنند نکاتی را خواهیم گفت.
و در پایان بحثی در مورد اینکه چگونه می‌توانیم
نمایش
(*representation*)
خوبی از داده‌ها تولید کنیم یا یاد بگیریم، خواهیم داشت.


<!-- Practicum -->

## تمرین عملی

<!--
We discuss the motivation for applying transformations to data points visualized in space. We talk about Linear Algebra and the application of linear and non-linear transformations. We discuss the use of visualization to understand the function and effects of these transformations. We walk through examples in a Jupyter Notebook and conclude with a discussion of functions represented by neural networks.
-->
داده‌ها را به صورت نقاطی که در فضای چند بعدی رسم شده‌اند فرض کنید.
در مورد انگیزه‌ی اعمال تبدیل
(*transformation*)
بر این داده‌ها صحبت خواهیم کردیم. 
در مورد جبر خطی و کاربرد تبدیل‌های خطی و غیرخطی مطالبی را خواهیم دید.
خواهیم دید که چقدر مصورسازی
(*visualization*)
می‌تواند به ما در فهم تبدیل‌ها کمک کند. چه در فهم خود آنها و چه در فهم اثر آنها.
در یک
*Jupyter Notebook*
چند مثال خواهیم دید. و در پایان در مورد توابعی بحث خواهیم کرد که می‌توانند توسط شبکه‌های عصبی ساخته شوند.
